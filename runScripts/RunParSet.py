#Run Parameter Sets
# Gets parameters from parameter file generated by SC_GenerateParamSet.py

import pandas as pd
import numpy as np
import random
import sys
import os
import math
#import ReadPFB as pfb
import pfio #Hoang's Parflow Read/Write Module
import time
import datetime

############# Functions ##############
# pfidbGen: pfidb generator
# createRunDir: creates parflow run directory for individual run
# getInputRow: get single set of run parameters
# processData: run post processing

# two different overall run behaviors
    # runSet runs a single simulation, in it's own folder
    # runSingleFolder, runs a series of simulations, in sequence, in the same folder.

def pfidbGen(runData):              # generates PFIDB File for Run given input data set
    '''creates pfidb file for parflow run'''

    # modify input data set
    runData = runData.drop(labels='n') # remove this values, not needed
    runData = runData.astype(str)

    # evaluate key and value lengths - these variables are needed for the pfidb file
    keyLengths=[len(i) for i in runData.keys()]
    valueLengths=[len(i) for i in np.array(runData)]

    # open the output file, then first save the number of keys
    outfn = 'test.pfidb'
    f = open(outfn,'a')
    f.write(str(len(runData.keys())))

    # loop through all the keys, add the key len, key, value len, and value to the pfidb file
    keyVals = np.array(runData)
    for i in range(len(keyVals)):
        f.write("\n")
        f.write(str(keyLengths[i]))
        f.write("\n")
        f.write(runData.keys()[i])
        f.write("\n")
        if keyVals[i] == 'nan': #nan values should be empty strings
            f.write("0")
            f.write("\n")
            f.write("\n")
        else:
            f.write(str(valueLengths[i]))
            f.write("\n")
            f.write(keyVals[i])
    f.close()

def createPumpFile(pumpVRate, pumpDepth_min, pumpDepth_max, dz, nz):
    '''creates parflow pumping file, given pumping rate (m3/hr), pumping depth range, and DZ/NZ'''

    # create pumping vector: for single column this is a simple vector the length of NZ
    pumpData = np.zeros(nz)

    # determine which layers will have pumping
    layTop = np.array([i for i in range(dz*(nz-1),-dz,-dz)])
    layBottom = layTop + dz
    layMiddle = (layTop + layBottom)/2.0
    pumpLayers = (layMiddle < pumpDepth_max) & (layMiddle > pumpDepth_min)
    pumpData[pumpLayers] = 1 # select these as the pumping layers, set to 1 for easy multiplication later

    # evalute pumping rate
    nlayers = pumpData.sum()
    pumpDepth = nlayers*dz
    assert pumpDepth == (pumpDepth_max - pumpDepth_min) #sanity check
    pumpRate = pumpVRate/1.12/1.12/pumpDepth # number of layers does not matter here, only the total depth, because the rate is 1/hr 
    pumpData = pumpData * pumpRate

    # save the file
    outfn = 'test_pump.txt'
    #pfio.pfwrite(pumpData, outfn, 0 , 0 , 0, 1, 1, nz)
    # pfio doesn't seem to like single column? or I"m doing it wrong... very possible
    with open(outfn,"w") as f:
        f.write('1 1 ')
        f.write(str(nz))
        for i in range(nz):
            f.write('\n')
            f.write(str(pumpData[i]))
    # convert text to pfb
    os.system('tclsh /glade/scratch/lmthatch/SCTests/runScripts/ConvertPFB2TXT 0 test_pump.txt test_pump.pfb')

def createRunDir(n): #(clmDir,currTestDir) input parameters ignored for now, assuming we're in the current directory and the clm directory has been copied into this folder
    '''create run directory for parflow files'''

    # create run directory
    runDir = 'test' + str(n)
    os.system('mkdir ' + runDir)

    # copy over clm driver files
    os.system("cp ../clm_input/drv_clmin.dat " + runDir)
    os.system("cp ../clm_input/drv_vegm.dat " + runDir)
    os.system("cp ../clm_input/drv_vegp.dat " + runDir)
    return runDir

def getInputRow(n,paramFile): # gets the input rows needed.
    '''get single parameter set from parameter file'''
    allPar = pd.read_csv(paramFile, delimiter=",", header=0) # room for improvement here, may be removed when 'well' parallelized to loop through parameter set
    linePar = allPar.iloc[n]
    return linePar

def getAllInputRows(paramFile): # gets all input rows
    '''get all parameter data sets'''
    allPar = pd.read_csv(paramFile, delimiter=",", header=0) # room for improvement here, may be removed when 'well' parallelized to loop through parameter set
    #linePar = allPar.iloc[n]
    return allPar

def processDataSC(rpars):
    '''process parflow output data'''
    n = rpars['n']
    nz = rpars['ComputationalGrid.NZ']
    runLen = rpars['TimingInfo.StopTime']

    # check if clm is being used, currently it's assum
    try:
        LSM = rpars['Solver.LSM'] == 'CLM'
    except:
        LSM = False

    # set variable names for parflow output files
    allLayers = list(np.arange(1,nz+1)) # list of all soil layers
    sat_colnames = ['sat_' + str(l) for l in allLayers]
    pres_colnames = ['press_' + str(l) for l in allLayers]

    if LSM:
        clmLayers = list(np.arange(1,rpars['Solver.CLM.RootZoneNZ']+1)) # list of all soil layers
        clm_colnames =  ['eflx_lh_tot','eflx_lwrad_out','eflx_sh_tot',
                            'eflx_soil_grnd','qflx_evap_tot','qflx_evap_grnd',
                            'qflx_evap_soi','qflx_evap_veg','qflx_trans_veg',
                            'qflx_infl','swe_out','t_grnd','qflx_qirr']
        clm_colnames.extend(['tsoil_' + str(l) for l in clmLayers]) # adds a soil temperature list

    # Loop through all files, combine all variables
    for k in range(1,(runLen+1)):
        #print(str(k))

        # set file names based on hour
        finSAT="test.out.satur.{:05d}.pfb".format(k)
        finPRES="test.out.press.{:05d}.pfb".format(k)

        # pull in the data, note this only works because it's a single cell model, if it were a 3-D model, would need a different method here, variables would need to be averaged or...
        dataSATpf = pfio.pfread(finSAT)
        dataSATDF = pd.DataFrame(np.transpose(dataSATpf[:,:,0]),columns=sat_colnames)
        dataPRESpf = pfio.pfread(finPRES)
        dataPRESDF = pd.DataFrame(np.transpose(dataPRESpf[:,:,0]),columns=pres_colnames)

        if LSM:
            finCLM="test.out.clm_output.{:05d}.C.pfb".format(k)
            dataCLMpf = pfio.pfread(finCLM)
            dataCLMDF = pd.DataFrame(np.transpose(dataCLMpf[:,:,0]),columns=clm_colnames)
            hourDat = pd.concat([dataCLMDF,dataSATDF,dataPRESDF], axis=1) # merge the datasets
        else:
            hourDat = pd.concat([dataSATDF,dataPRESDF], axis=1) # merge the datasets

        if k==1:
            allData = hourDat
        else:
            allData = allData.append(hourDat)

    # add a datetime to the data


    # calculate storage
    # storage is dx*dy*dz*press*Ss*Sat + Sat*porosity
    dx = rpars['ComputationalGrid.DX']
    dy = rpars['ComputationalGrid.DY']
    dz = rpars['ComputationalGrid.DY']
    por = rpars['Geom.domain.Porosity.Value']
    ss = rpars['Geom.domain.SpecificStorage.Value']
    sto_colnames = ['sto_' + str(l) for l in allLayers]

    for i in range(nz):
        sat = allData[sat_colnames[i]]
        pres = allData[pres_colnames[i]]
        sto =  pres.multiply(sat) * dx * dy * dz * ss + sat * por * dx * dy * dz
        allData[sto_colnames[i]] = sto

    # total storage over time
    

    # save all data
    fileOut = '../FullRunData/FullRunData_test' + str(n) + ".csv"
    #fileOut = 'FullRunData/FullRunData_test' + str(n) + ".csv"
    allData.to_csv(fileOut,index=False) 


    if LSM:
        fluxVars = ['qflx_evap_tot','qflx_evap_grnd',
                    'qflx_evap_soi','qflx_evap_veg','qflx_trans_veg',
                    'qflx_infl','qflx_qirr']
        fluxData = allData[fluxVars] #%>% summarise_all(sum)
        fluxTots = fluxData.agg(['sum'])
        fluxTots.columns = ['Total_m_' + str(s) for s in fluxTots.columns]

        sweData = allData[['swe_out']]
        sweTots = sweData.agg(['mean'])
        sweTots.columns = ['Mean_m_' + str(s) for s in sweTots.columns]

        tempVars = ['t_grnd']
        tempVars.extend(['tsoil_' + str(l) for l in clmLayers])
        tempData = allData[tempVars]
        tempTots = tempData.agg(['mean'])
        tempTots.columns = ['Mean_' + s for s in tempTots.columns]
    
    chngStoData = allData[sto_colnames]
    initSto = chngStoData.iloc[0]
    finalSto = chngStoData.iloc[-1]
    diffSto = finalSto.subtract(initSto)
    chngStoTots = pd.DataFrame(diffSto).transpose()
    chngStoTots.columns = ['Chng_' + s for s in chngStoTots.columns] 



    #chngSData = allData[sat_colnames]
    #initialSAT = chngSData.iloc[0]
    #finalSAT = chngSData.iloc[len(chngSData)-1]
    #diffSAT = finalSAT.subtract(initialSAT)
    #chngSTots = pd.DataFrame(diffSAT).transpose()
    #chngSTots.columns = ['Chng_' + s for s in chngSTots.columns]

    #chngPData = allData[pres_colnames]
    #initialPRES = chngPData.iloc[0]
    #finalPRES = chngPData.iloc[len(chngPData)-1]
    #diffPRES = finalPRES.subtract(initialPRES)
    #chngPTots = pd.DataFrame(diffPRES).transpose()
    #chngPTots.columns = ['Chng_' + s for s in chngPTots.columns]


    # if clmlay < 10 or nlay < 1000:
    #     blankDatSat = np.empty((1,(1000 - nlay)))
    #     blankDatSat[:] = np.NaN
    #     blankDatTemp = np.empty((1,(10 - clmlay)))
    #     blankDatTemp[:] = np.NaN

    #     blankSATDF = pd.DataFrame(blankDatSat)
    #     blankPRESDF = blankSATDF
    #     blankSATDF.columns = [ 'Chng_sat_' + str(l) for l in list(np.arange(nlay+1,1001))]
    #     blankPRESDF.columns = [ 'Chng_press_' + str(l) for l in list(np.arange(nlay+1,1001))]
    #     blankTempDF = pd.DataFrame(blankDatTemp)
    #     blankTempDF.columns = [ 'Mean_' + str(l) for l in list(np.arange(clmlay+1,11))]

    #     # add it to dataframes
    #     tempTots = pd.concat([tempTots,blankTempDF], axis=1)
    #     chngPTots = pd.concat([chngPTots,blankPRESDF], axis=1)
    #     chngSTots = pd.concat([chngSTots,blankSATDF], axis=1)

    # merge data frames
    chngStoTots=chngStoTots.reset_index(drop=True)

    if LSM:
        fluxTots=fluxTots.reset_index(drop=True)
        sweTots=sweTots.reset_index(drop=True)    
        tempTots=tempTots.reset_index(drop=True)
        singleLineOut = pd.concat([fluxTots,sweTots,tempTots,chngStoTots], axis=1) # merge the datasets
    else:
        singleLineOut = chngStoTots
    #chngPTots=chngPTots.reset_index(drop=True)
    #chngSTots=chngSTots.reset_index(drop=True)    
    #merge1 = pd.concat([fluxTots,sweTots],axis=1,ignore_index = True)
    #merge2 = pd.concat([merge1,tempTots],axis=1,ignore_index = True)
    #merge3 = pd.concat([merge2,chngPTots],axis=1,ignore_index = True)
    #singleLineOut = pd.concat([merge2,chngSTots],axis=1,ignore_index = True)
    
    fileOut = '../SingleLineOutput/SingleLineOutput_test' + str(n) + ".csv"
    singleLineOut.to_csv(fileOut,index=False)
   
def runSet(parLine, parameterFN):
    #parameterFN = "ParameterSets_AutoGenPY.csv"
    # create your run directory
    newRunDir = createRunDir(parLine)

    # get your run data
    runParameters = getInputRow(parLine, parameterFN)
    #runParameters['TimingInfo.StopTime'] = 500 
    # navigate to this new folder your created
    os.chdir(newRunDir)

    # create your pfidb file
    pfidbGen(runParameters)

    # run your program
    os.system("$PARFLOW_DIR/bin/parflow test > parflow.test.log")

    # process the data
    #nclm = runParameters['Solver.CLM.RootZoneNZ']
    #totalLayers = runParameters['ComputationalGrid.NZ']
    #testn = runParameters['n']
    #runLen = runParameters['TimingInfo.StopTime']
    processDataSC(runParameters)

    os.system('rm test.pfidb')

    # delete your folder
    #os.chdir('../')
    #os.system('rm ' + newRunDir)

    return 'parameter set complete: ' + str(parLine)

def runSingleFolder(runset): # this is for running in parallel w/ a set number of maximum folders running at a time.
    # first we'll create a single run folder

    runset = runset - 1 # the GNU parallel is running a sequence from 1-5 but python wants 0-4, correct here.
    print('Running Folder ' + str(runset))
    newRunDir = createRunDir(runset)

    parameterFN = "ParameterSets_AutoGenPY_" + str(runset) + ".csv"

    # get all run parameters
    allPar = getAllInputRows(parameterFN)
    nsets = len(allPar.index)

    os.chdir(newRunDir)

    outfn = '../runTimes_' + str(runset) + '.csv'

    for currset in range(nsets):
        print('Running Set ' + str(currset))

        # get current parameter set
        runParameters = allPar.iloc[currset]

        # create your pfidb file
        pfidbGen(runParameters)
        #print('PfidbGenerated')

        # run your program
        #os.system("$PARFLOW_DIR/bin/parflow test > parflow.test.log")
        start = time.time()
        os.system("$PARFLOW_DIR/bin/parflow test  > parflow.test.log")
        end = time.time()

        # save runtime
        totaltime = end - start
        f = open(outfn,'a')
        testn = runParameters['n']
        f.write(str(testn) + "," + str(totaltime))
        f.write("\n")
        f.close()

        #print('Parflow Run Done')

        # process the data
        #nclm = runParameters['Solver.CLM.RootZoneNZ']
        #totalLayers = runParameters['ComputationalGrid.NZ']
        #runLen = runParameters['TimingInfo.StopTime']
        processDataSC(runParameters)
        #print('Processing Complete')
        #print('Deleting old pfidb file')
        os.system('rm test.pfidb')

    # delete the directory when all runs have been completed
    os.chdir('../')
    os.system('rm -r ' + newRunDir)

def main():
    runset = int(sys.argv[1])
    runSingleFolder(runset)

    # old def main -> When file creation not restricted (no go on Cheyenne)
    # runLine = int(sys.argv[1],"ParameterSets_AutoGenPY.csv")
    # runSet(runLine)

if __name__ == "__main__":

    main() # main is passed the line in the parameter file that you want to pull

